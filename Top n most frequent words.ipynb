{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the file contents of the dfs\n",
    "hdfs dfs -ls\n",
    "#use 'hdfs dfs copyFromLocal' to transfer files from local to dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/anaconda/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#get lines from the input\n",
    "for line in sys.stdin:\n",
    "    #strip/trim any leading or trailing spaces\n",
    "    line = line.strip()\n",
    "    #split the line into a words array/list\n",
    "    words = line.split()\n",
    "    #print out each word in the words list to output\n",
    "    for word in words:\n",
    "        print(f\"{word}\\t{1}\")\n",
    "        \n",
    "#1st mapper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/anaconda/bin/python\n",
    "import sys\n",
    "\n",
    "prev_key = None\n",
    "prev_count = 0\n",
    "word = None\n",
    "curr_count = 0\n",
    "#get a line from input\n",
    "for line in sys.stdin:\n",
    "    #strip leading/trailing spaces\n",
    "    line = line.strip()\n",
    "    #split the line with tab as the separator\n",
    "    #with atmost one split\n",
    "    word, curr_count = line.split('\\t',1)\n",
    "    \n",
    "    #try casting the curr_count from string to int\n",
    "    try:\n",
    "        curr_count = int (curr_count)\n",
    "    \n",
    "    #if the casting doesn't work, just skip the word \n",
    "    #and go to next iteration of the loop\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    #if previous word is same as current word, just increment existing count\n",
    "    #with new count\n",
    "    if prev_key == word:\n",
    "        prev_count += curr_count\n",
    "    #if the previous word is not the same as current\n",
    "    #write the prev word and its count to output\n",
    "    else:\n",
    "        #if a previous key already exists\n",
    "        #(mostly to check if prev_key had already been assigned. If its not assigned, it evaluates to false)\n",
    "        if prev_key:\n",
    "            print(f\"{prev_key}\\t{prev_count}\")\n",
    "        #make the current word and count assign to prev\n",
    "        prev_key = word\n",
    "        prev_count = curr_count\n",
    "\n",
    "#print the last word\n",
    "print(f\"{prev_key}\\t{prev_count}\")\n",
    "        \n",
    "#first reducer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command to run\n",
    "hadoop jar {location of streaming jar} -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \n",
    "-input {location of the input in the hdfs directory} -output {location of where the output should be placed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/anaconda/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "    #make count as the key\n",
    "    print(f\"{count}\\t{word}\")\n",
    "    \n",
    "#second mapper that takes input from the first mapreduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\tNone\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/anaconda/bin/python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "prev_key = None\n",
    "wlist = None\n",
    "curr_key = None\n",
    "\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    curr_key, word = line.split('\\t', 1)\n",
    "    \n",
    "    try:\n",
    "        curr_key = int (curr_key)\n",
    "        \n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    if prev_key == curr_key:\n",
    "        #add to the word list when both words have the same count\n",
    "        wlist.append(word)\n",
    "        \n",
    "    else:\n",
    "        #output the prev key with its associated list\n",
    "        print(f\"{prev_key}\\t{wlist}\")\n",
    "        #create new list\n",
    "        wlist = []\n",
    "        #assign current key (count) as prev key\n",
    "        prev_key = curr_key\n",
    "        #append the current word to word list\n",
    "        wlist.append(word)\n",
    "     \n",
    "print(f\"{prev_key}\\t{wlist}\")\n",
    "\n",
    "#second reducer that takes the second mapper output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jardir = location of the hadoop-streaming.jar\n",
    "#hdinp = location of input in the dfs\n",
    "hadoop jar $jardir -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=-k1,1nr -files mapper2.py,reducer2.py -mapper mapper2.py \\\n",
    "-reducer reducer2.py -input $hdinp -output rating1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
